{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeRateDogs Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    ">In this report, wrangling and analysis was carried out on the tweet archive of a Twitter user, @dog_rates, also known as WeRateDogs. This Twitter account rates people's dogs with humorous comments.\n",
    "\n",
    "The wrangling steps taken:\n",
    "- Data gathering\n",
    "- Assessing data\n",
    "- Cleaning data\n",
    "\n",
    "Wrangling tasks completed in each of these steps are explained in the following paragraphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Gathering\n",
    "> Data was gathered from 3 different sources, outlined below;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Twitter archive file:__\n",
    "> This archive contains basic tweet data (tweet ID, timestamp, text, etc.) from year 2015 up to August 1, 2017. These data is populated in a csv file saved as twitter-archive-enhanced.csv, was downloaded manually and loaded into a pandas’ DataFrame.\n",
    "\n",
    "- __The tweet image predictions:__\n",
    "> This dataset contains an algorithm's top three dog breed predictions along with tweet ID, image URL, and the image number that corresponded to the most confident prediction of dog breed for each dog image from the WeRateDogs Enhanced Twitter Archive. The Data was downloaded programmatically using the Python Requests library from the URL address into a tsv file. The content of image-predictions.tsv file is then loaded into the pandas' DataFrame.\n",
    "\n",
    "- __Twitter API File:__\n",
    "> This file contains tweet id, favorite count and retweet count etc. This was downloaded programmatically using API credentials provided by Twitter, and saved into a tweet-json.txt. Subsequently, the tweet-json.txt file was loaded into a pandas’ DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assessing data\n",
    "> After gathering, the dataset was assessed for tidiness and quality as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Enhanced Twitter Archive__\n",
    "> A sample dataset was visually assessed. A summary of the datatypes was also assesed. Then, IDs were checked for duplicates.\n",
    "\n",
    " > Expanded URLs were assessed visually and programmatically for the existence of two or more URLs in one cell\n",
    "\n",
    " > Name of dog column was assessed programmatically. All tweets were checked for dogs with more than one dog stage assigned.\n",
    "\n",
    " > Rating denominators and numerators were assessed visually. Programmatic checks was carried out on the text column.\n",
    "\n",
    "- __Image Predictions Data__\n",
    "> A sample of data was visually assessed. A summary of the datatypes was also assessed.\n",
    " > The jpg_url column was checked for duplicates\n",
    "\n",
    " > The 1st image prediction results was explored.\n",
    "\n",
    "- __Twitter API Data__\n",
    " > A sample of the dataset was assessed visually. Summary of the datatypes was also assessed.\n",
    " > Checks were carried out to ascertain if the API Data contains Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning data\n",
    "> Quality and tidiness issues identified in the Assessing Data phase were cleaned using pandas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Enhanced Twitter Archive__\n",
    "> A copy of the dataset was created. Tweets, Replies, Retweets and other redundant columns were removed from the dataframe.\n",
    "\n",
    " > Wrong Dog names were cleaned\n",
    " \n",
    " > Dog 'stage' classification which was broken into four separate columns, was merged into one column.\n",
    " \n",
    " > Source column which contains HTML was redefined by extracting sources from the HTML.\n",
    " \n",
    " > 639 expanded URLs which contain more than one URL address was recorded. Correct links was thus built by using the tweet id field.\n",
    " \n",
    " > Timestamp column was converted to DateTime.\n",
    " \n",
    " > Float ratings were properly gathered.\n",
    " \n",
    "- __Image Predictions__\n",
    "> A copy of the dataset was created.\n",
    "\n",
    " > 66 image url duplicates were removed.\n",
    " \n",
    " > A column each for prediction and confidence level was created.\n",
    " > Dog breeds column was cleaned.\n",
    " \n",
    " > The clean data was merged with Twitter archive Data using tweet ID column\n",
    " \n",
    "- __Twitter API Data__\n",
    "> A copy of the original dataset was created\n",
    "\n",
    " > Text range column was split into two separate columns\n",
    " \n",
    " > Lastly,, this dataset was merged with the resulting merged data in Image prediction, and exported to twitter_archive_master.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
